{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dff2c11",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Networks (PINNs) - Comprehensive Theory\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Physics-Informed Neural Networks (PINNs) are a class of neural networks that incorporate physical laws, described by partial differential equations (PDEs), directly into the training process. This approach bridges machine learning with scientific computing.\n",
    "\n",
    "### Key Innovation\n",
    "Instead of purely data-driven learning, PINNs embed the governing physics equations as soft constraints in the loss function, allowing the network to learn solutions that satisfy both data and physical laws.\n",
    "\n",
    "## 2. Mathematical Foundation\n",
    "\n",
    "### 2.1 General PDE Form\n",
    "\n",
    "Consider a general PDE in space-time domain:\n",
    "\n",
    "```\n",
    "∂u/∂t + N[u; λ] = 0,  x ∈ Ω, t ∈ [0, T]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `u(x, t)` is the solution we seek\n",
    "- `N[·]` is a nonlinear differential operator\n",
    "- `λ` are parameters of the PDE\n",
    "- `Ω` is the spatial domain\n",
    "\n",
    "### 2.2 Boundary and Initial Conditions\n",
    "\n",
    "**Initial Condition (IC):**\n",
    "```\n",
    "u(x, 0) = u₀(x)\n",
    "```\n",
    "\n",
    "**Boundary Conditions (BC):**\n",
    "```\n",
    "B[u] = g(x, t)  on ∂Ω\n",
    "```\n",
    "\n",
    "## 3. PINN Architecture\n",
    "\n",
    "### 3.1 Neural Network Approximation\n",
    "\n",
    "The solution u(x, t) is approximated by a deep neural network:\n",
    "\n",
    "```\n",
    "u(x, t) ≈ û(x, t; θ)\n",
    "```\n",
    "\n",
    "Where θ represents all trainable parameters (weights and biases).\n",
    "\n",
    "### 3.2 Network Structure\n",
    "\n",
    "```\n",
    "Input Layer: [x, t]\n",
    "    ↓\n",
    "Hidden Layer 1: Linear → Activation\n",
    "    ↓\n",
    "Hidden Layer 2: Linear → Activation\n",
    "    ↓\n",
    "    ...\n",
    "    ↓\n",
    "Hidden Layer N: Linear → Activation\n",
    "    ↓\n",
    "Output Layer: u(x, t)\n",
    "```\n",
    "\n",
    "**Common Activation Functions:**\n",
    "- Hyperbolic tangent: tanh(x)\n",
    "- Sine: sin(x)\n",
    "- Swish: x · sigmoid(x)\n",
    "\n",
    "## 4. Automatic Differentiation\n",
    "\n",
    "PINNs leverage automatic differentiation to compute derivatives of the network output with respect to inputs:\n",
    "\n",
    "```\n",
    "∂û/∂t, ∂û/∂x, ∂²û/∂x², etc.\n",
    "```\n",
    "\n",
    "This allows the network to evaluate the PDE residual at any point without numerical differentiation.\n",
    "\n",
    "### Example in PyTorch:\n",
    "```python\n",
    "u = network(x, t)\n",
    "u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), \n",
    "                          create_graph=True)[0]\n",
    "u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), \n",
    "                          create_graph=True)[0]\n",
    "```\n",
    "\n",
    "## 5. Loss Function Formulation\n",
    "\n",
    "The total loss is a weighted sum of multiple components:\n",
    "\n",
    "```\n",
    "L_total = w_data · L_data + w_pde · L_pde + w_ic · L_ic + w_bc · L_bc\n",
    "```\n",
    "\n",
    "### 5.1 Data Loss\n",
    "```\n",
    "L_data = (1/N_data) Σ |û(x_i, t_i) - u_i|²\n",
    "```\n",
    "\n",
    "### 5.2 PDE Residual Loss\n",
    "```\n",
    "L_pde = (1/N_pde) Σ |∂û/∂t + N[û]|²\n",
    "```\n",
    "\n",
    "### 5.3 Initial Condition Loss\n",
    "```\n",
    "L_ic = (1/N_ic) Σ |û(x_i, 0) - u₀(x_i)|²\n",
    "```\n",
    "\n",
    "### 5.4 Boundary Condition Loss\n",
    "```\n",
    "L_bc = (1/N_bc) Σ |B[û](x_i, t_i) - g(x_i, t_i)|²\n",
    "```\n",
    "\n",
    "## 6. Training Process\n",
    "\n",
    "### 6.1 Collocation Points\n",
    "\n",
    "PINNs use collocation points sampled from the domain:\n",
    "\n",
    "- **Data points**: Where measurements are available\n",
    "- **PDE collocation points**: Interior points where PDE must be satisfied\n",
    "- **Boundary points**: Points on domain boundaries\n",
    "- **Initial points**: Points at t = 0\n",
    "\n",
    "### 6.2 Optimization\n",
    "\n",
    "**Common Optimizers:**\n",
    "1. **Adam**: Fast convergence in early stages\n",
    "2. **L-BFGS**: Better convergence for fine-tuning\n",
    "\n",
    "**Two-Stage Training:**\n",
    "```\n",
    "Stage 1: Adam optimizer (epochs: 10,000-50,000)\n",
    "Stage 2: L-BFGS optimizer (iterations: 1,000-5,000)\n",
    "```\n",
    "\n",
    "### 6.3 Training Algorithm\n",
    "\n",
    "```\n",
    "1. Initialize network parameters θ\n",
    "2. For each epoch:\n",
    "   a. Sample collocation points\n",
    "   b. Forward pass: compute û and derivatives\n",
    "   c. Compute loss components\n",
    "   d. Backward pass: compute gradients\n",
    "   e. Update parameters: θ ← θ - α∇L\n",
    "3. Repeat until convergence\n",
    "```\n",
    "\n",
    "## 7. Advantages of PINNs\n",
    "\n",
    "1. **Data Efficiency**: Can learn from sparse data by incorporating physics\n",
    "2. **Mesh-Free**: No need for discretization of space-time domain\n",
    "3. **Inverse Problems**: Can infer unknown parameters in PDEs\n",
    "4. **Continuous Solution**: Provides solution at any point in domain\n",
    "5. **Physical Consistency**: Solutions respect conservation laws\n",
    "\n",
    "## 8. Challenges and Solutions\n",
    "\n",
    "### 8.1 Training Difficulties\n",
    "\n",
    "**Problem**: Imbalanced gradients between loss terms\n",
    "\n",
    "**Solution**: \n",
    "- Adaptive loss weighting\n",
    "- Gradient normalization\n",
    "- Learning rate scheduling\n",
    "\n",
    "### 8.2 Spectral Bias\n",
    "\n",
    "**Problem**: NNs learn low-frequency features first\n",
    "\n",
    "**Solution**:\n",
    "- Fourier feature mapping\n",
    "- Periodic activation functions\n",
    "- Multi-scale architectures\n",
    "\n",
    "### 8.3 Hard Constraints\n",
    "\n",
    "**Problem**: Soft constraints may not be exactly satisfied\n",
    "\n",
    "**Solution**:\n",
    "- Penalty methods with high weights\n",
    "- Transform network output to enforce constraints\n",
    "- Sequential training strategies\n",
    "\n",
    "## 9. Extensions and Variants\n",
    "\n",
    "### 9.1 Conservative PINNs (cPINNs)\n",
    "Enforce conservation laws exactly through network architecture\n",
    "\n",
    "### 9.2 Variational PINNs (VPINNs)\n",
    "Use variational formulation for better accuracy\n",
    "\n",
    "### 9.3 Extended PINNs (XPINNs)\n",
    "Domain decomposition for large-scale problems\n",
    "\n",
    "### 9.4 Bayesian PINNs\n",
    "Quantify uncertainty in predictions\n",
    "\n",
    "## 10. Key Hyperparameters\n",
    "\n",
    "| Parameter | Typical Range | Description |\n",
    "|-----------|---------------|-------------|\n",
    "| Hidden Layers | 4-8 | Depth of network |\n",
    "| Neurons/Layer | 20-50 | Width of network |\n",
    "| Learning Rate | 1e-3 to 1e-4 | Step size for optimizer |\n",
    "| Collocation Points | 10,000-50,000 | Points for PDE evaluation |\n",
    "| Loss Weights | 0.1-10 | Balance between loss terms |\n",
    "| Activation | tanh, sin | Nonlinear function |\n",
    "\n",
    "## 11. Practical Implementation Tips\n",
    "\n",
    "1. **Normalization**: Scale inputs and outputs to [-1, 1] or [0, 1]\n",
    "2. **Initialization**: Use Xavier or He initialization\n",
    "3. **Monitoring**: Track individual loss components\n",
    "4. **Visualization**: Plot predictions regularly during training\n",
    "5. **Residual Plotting**: Visualize where PDE is violated\n",
    "6. **Convergence**: Use relative L2 error for assessment\n",
    "\n",
    "## 12. Applications\n",
    "\n",
    "- **Fluid Dynamics**: Navier-Stokes, Euler equations\n",
    "- **Heat Transfer**: Diffusion, convection problems\n",
    "- **Solid Mechanics**: Elasticity, wave propagation\n",
    "- **Quantum Mechanics**: Schrödinger equation\n",
    "- **Finance**: Black-Scholes equation\n",
    "- **Biology**: Reaction-diffusion systems\n",
    "\n",
    "## 13. Performance Metrics\n",
    "\n",
    "### L2 Relative Error\n",
    "```\n",
    "Error = ||u_true - u_pred||₂ / ||u_true||₂\n",
    "```\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "```\n",
    "MSE = (1/N) Σ (u_true - u_pred)²\n",
    "```\n",
    "\n",
    "### Maximum Absolute Error\n",
    "```\n",
    "Max Error = max|u_true - u_pred|\n",
    "```\n",
    "\n",
    "## 14. References and Further Reading\n",
    "\n",
    "1. Raissi, M., Perdikaris, P., & Karniadakis, G. E. (2019). Physics-informed neural networks\n",
    "2. Karniadakis, G. E., et al. (2021). Physics-informed machine learning\n",
    "3. Cuomo, S., et al. (2022). Scientific machine learning through physics-informed neural networks\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**: Apply this theory to implement PINNs for Burger's equation, which combines nonlinear convection with diffusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a2dc1a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
