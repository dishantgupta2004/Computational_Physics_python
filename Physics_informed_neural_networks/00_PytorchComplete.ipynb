{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cpu\n",
      "CUDA available: False\n",
      "CUDA version: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Complete Comprehensive Guide\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to PyTorch](#1-introduction-to-pytorch)\n",
    "2. [Core Concepts: Tensors](#2-core-concepts-tensors)\n",
    "3. [Autograd: Automatic Differentiation](#3-autograd-automatic-differentiation)\n",
    "4. [Neural Networks with torch.nn](#4-neural-networks-with-torchnn)\n",
    "5. [Optimization](#5-optimization)\n",
    "6. [Data Loading and Processing](#6-data-loading-and-processing)\n",
    "7. [Training Pipeline](#7-training-pipeline)\n",
    "8. [Advanced Features](#8-advanced-features)\n",
    "9. [PyTorch Variants and Ecosystem](#9-pytorch-variants-and-ecosystem)\n",
    "10. [Best Practices and Tips](#10-best-practices-and-tips)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction to PyTorch\n",
    "\n",
    "### What is PyTorch?\n",
    "\n",
    "PyTorch is an open-source machine learning framework developed by Meta AI (Facebook). It provides:\n",
    "- Dynamic computational graphs (define-by-run)\n",
    "- Automatic differentiation\n",
    "- GPU acceleration\n",
    "- Pythonic interface\n",
    "- Rich ecosystem of tools\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "1. **Dynamic Computation Graphs**: Build and modify networks on the fly\n",
    "2. **Eager Execution**: Immediate operation execution (easier debugging)\n",
    "3. **Pythonic**: Feels like native Python, easy to learn\n",
    "4. **Strong Community**: Extensive resources and libraries\n",
    "5. **Research-Friendly**: Rapid prototyping and experimentation\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "# CPU only\n",
    "pip install torch torchvision torchaudio\n",
    "\n",
    "# CUDA 11.8\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# CUDA 12.1\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# For Apple Silicon (M1/M2)\n",
    "pip install torch torchvision torchaudio\n",
    "```\n",
    "\n",
    "\n",
    "## 2. Core Concepts: Tensors\n",
    "\n",
    "### What are Tensors?\n",
    "\n",
    "Tensors are multi-dimensional arrays, similar to NumPy arrays but with GPU support and automatic differentiation.\n",
    "\n",
    "### Tensor Creation\n",
    "\n",
    "```python\n",
    "# From Python lists\n",
    "tensor_from_list = torch.tensor([1, 2, 3, 4])\n",
    "tensor_2d = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# Zeros and ones\n",
    "zeros = torch.zeros(3, 4)  # 3x4 tensor of zeros\n",
    "ones = torch.ones(2, 3, 4)  # 2x3x4 tensor of ones\n",
    "\n",
    "# Random tensors\n",
    "rand_tensor = torch.rand(3, 4)  # Uniform [0, 1)\n",
    "randn_tensor = torch.randn(3, 4)  # Normal distribution N(0,1)\n",
    "\n",
    "# Range tensors\n",
    "arange_tensor = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]\n",
    "linspace_tensor = torch.linspace(0, 10, 5)  # 5 points from 0 to 10\n",
    "\n",
    "# Identity matrix\n",
    "identity = torch.eye(4)\n",
    "\n",
    "# Like operations (same shape as another tensor)\n",
    "x = torch.rand(3, 4)\n",
    "zeros_like = torch.zeros_like(x)\n",
    "ones_like = torch.ones_like(x)\n",
    "\n",
    "# Full tensors\n",
    "full_tensor = torch.full((3, 4), 7)  # Fill with value 7\n",
    "```\n",
    "\n",
    "### Tensor Attributes\n",
    "\n",
    "```python\n",
    "tensor = torch.rand(3, 4, 5)\n",
    "\n",
    "print(f\"Shape: {tensor.shape}\")  # torch.Size([3, 4, 5])\n",
    "print(f\"Size: {tensor.size()}\")  # Same as shape\n",
    "print(f\"Dtype: {tensor.dtype}\")  # torch.float32\n",
    "print(f\"Device: {tensor.device}\")  # cpu or cuda:0\n",
    "print(f\"Number of dimensions: {tensor.ndim}\")  # 3\n",
    "print(f\"Total elements: {tensor.numel()}\")  # 60\n",
    "```\n",
    "\n",
    "### Data Types\n",
    "\n",
    "```python\n",
    "# Common data types\n",
    "float_tensor = torch.tensor([1, 2, 3], dtype=torch.float32)  # float32\n",
    "double_tensor = torch.tensor([1, 2, 3], dtype=torch.float64)  # float64\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)  # int32\n",
    "long_tensor = torch.tensor([1, 2, 3], dtype=torch.long)  # int64\n",
    "bool_tensor = torch.tensor([True, False], dtype=torch.bool)\n",
    "\n",
    "# Type conversion\n",
    "x = torch.tensor([1, 2, 3])\n",
    "x_float = x.float()  # to float32\n",
    "x_double = x.double()  # to float64\n",
    "x_int = x_float.int()  # to int32\n",
    "x_long = x.long()  # to int64\n",
    "```\n",
    "\n",
    "### Tensor Operations\n",
    "\n",
    "#### Basic Arithmetic\n",
    "\n",
    "```python\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# Element-wise operations\n",
    "c = a + b  # [5, 7, 9]\n",
    "c = a - b  # [-3, -3, -3]\n",
    "c = a * b  # [4, 10, 18]\n",
    "c = a / b  # [0.25, 0.4, 0.5]\n",
    "\n",
    "# In-place operations (modify tensor in place)\n",
    "a.add_(b)  # a = a + b\n",
    "a.mul_(2)  # a = a * 2\n",
    "a.sub_(1)  # a = a - 1\n",
    "\n",
    "# Power and exponentials\n",
    "c = torch.pow(a, 2)  # a^2\n",
    "c = torch.exp(a)  # e^a\n",
    "c = torch.log(a)  # ln(a)\n",
    "c = torch.sqrt(a)  # sqrt(a)\n",
    "```\n",
    "\n",
    "#### Matrix Operations\n",
    "\n",
    "```python\n",
    "# Matrix multiplication\n",
    "A = torch.rand(3, 4)\n",
    "B = torch.rand(4, 5)\n",
    "C = torch.matmul(A, B)  # or A @ B, shape: (3, 5)\n",
    "\n",
    "# Batch matrix multiplication\n",
    "batch_A = torch.rand(10, 3, 4)  # 10 matrices of size 3x4\n",
    "batch_B = torch.rand(10, 4, 5)  # 10 matrices of size 4x5\n",
    "batch_C = torch.bmm(batch_A, batch_B)  # shape: (10, 3, 5)\n",
    "\n",
    "# Transpose\n",
    "A_T = A.t()  # 2D transpose\n",
    "A_T = A.transpose(0, 1)  # Transpose dimensions 0 and 1\n",
    "\n",
    "# Permute (multi-dimensional transpose)\n",
    "x = torch.rand(2, 3, 4, 5)\n",
    "x_perm = x.permute(0, 3, 1, 2)  # shape: (2, 5, 3, 4)\n",
    "```\n",
    "\n",
    "#### Reshaping Operations\n",
    "\n",
    "```python\n",
    "x = torch.rand(2, 3, 4)\n",
    "\n",
    "# View (must be contiguous in memory)\n",
    "x_view = x.view(6, 4)  # shape: (6, 4)\n",
    "x_view = x.view(-1, 4)  # -1 infers the dimension\n",
    "\n",
    "# Reshape (works with non-contiguous tensors)\n",
    "x_reshape = x.reshape(2, 12)\n",
    "\n",
    "# Squeeze and unsqueeze\n",
    "x = torch.rand(1, 3, 1, 4)\n",
    "x_squeezed = x.squeeze()  # Remove all dimensions of size 1: (3, 4)\n",
    "x_squeezed_dim = x.squeeze(0)  # Remove dimension 0: (3, 1, 4)\n",
    "\n",
    "x = torch.rand(3, 4)\n",
    "x_unsqueezed = x.unsqueeze(0)  # Add dimension at position 0: (1, 3, 4)\n",
    "x_unsqueezed = x.unsqueeze(-1)  # Add at last position: (3, 4, 1)\n",
    "\n",
    "# Flatten\n",
    "x = torch.rand(2, 3, 4)\n",
    "x_flat = x.flatten()  # shape: (24,)\n",
    "x_flat = x.flatten(start_dim=1)  # shape: (2, 12)\n",
    "```\n",
    "\n",
    "#### Concatenation and Stacking\n",
    "\n",
    "```python\n",
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "c = torch.rand(2, 3)\n",
    "\n",
    "# Concatenate along existing dimension\n",
    "concat_0 = torch.cat([a, b, c], dim=0)  # shape: (6, 3)\n",
    "concat_1 = torch.cat([a, b, c], dim=1)  # shape: (2, 9)\n",
    "\n",
    "# Stack (creates new dimension)\n",
    "stack_0 = torch.stack([a, b, c], dim=0)  # shape: (3, 2, 3)\n",
    "stack_1 = torch.stack([a, b, c], dim=1)  # shape: (2, 3, 3)\n",
    "```\n",
    "\n",
    "#### Indexing and Slicing\n",
    "\n",
    "```python\n",
    "x = torch.rand(4, 5, 6)\n",
    "\n",
    "# Basic indexing\n",
    "element = x[0, 1, 2]  # Single element\n",
    "row = x[0]  # First matrix: shape (5, 6)\n",
    "column = x[:, :, 0]  # First column of all matrices: shape (4, 5)\n",
    "\n",
    "# Slicing\n",
    "slice_x = x[1:3, :, 2:5]  # shape: (2, 5, 3)\n",
    "\n",
    "# Boolean indexing\n",
    "mask = x > 0.5\n",
    "selected = x[mask]  # All elements > 0.5\n",
    "\n",
    "# Advanced indexing\n",
    "indices = torch.tensor([0, 2, 3])\n",
    "selected_rows = x[indices]  # Select rows 0, 2, 3\n",
    "```\n",
    "\n",
    "#### Reduction Operations\n",
    "\n",
    "```python\n",
    "x = torch.rand(3, 4, 5)\n",
    "\n",
    "# Sum\n",
    "total_sum = x.sum()  # Sum all elements\n",
    "sum_dim0 = x.sum(dim=0)  # Sum over dimension 0: shape (4, 5)\n",
    "sum_keepdim = x.sum(dim=1, keepdim=True)  # shape: (3, 1, 5)\n",
    "\n",
    "# Mean\n",
    "mean = x.mean()\n",
    "mean_dim = x.mean(dim=2)\n",
    "\n",
    "# Max and Min\n",
    "max_val = x.max()\n",
    "max_dim, max_indices = x.max(dim=1)  # Returns values and indices\n",
    "\n",
    "min_val = x.min()\n",
    "min_dim, min_indices = x.min(dim=1)\n",
    "\n",
    "# Standard deviation and variance\n",
    "std = x.std()\n",
    "var = x.var()\n",
    "\n",
    "# Argmax and Argmin\n",
    "argmax = x.argmax()  # Index of maximum element\n",
    "argmax_dim = x.argmax(dim=0)  # Indices along dimension 0\n",
    "```\n",
    "\n",
    "### GPU Operations\n",
    "\n",
    "```python\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')  # GPU 0\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Move tensors to GPU\n",
    "x_cpu = torch.rand(3, 4)\n",
    "x_gpu = x_cpu.to(device)  # Copy to GPU\n",
    "# or\n",
    "x_gpu = x_cpu.cuda()\n",
    "\n",
    "# Move back to CPU\n",
    "x_back = x_gpu.cpu()\n",
    "\n",
    "# Create directly on GPU\n",
    "x_gpu = torch.rand(3, 4, device=device)\n",
    "\n",
    "# Multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    x_gpu1 = torch.rand(3, 4, device='cuda:1')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Autograd: Automatic Differentiation\n",
    "\n",
    "### What is Autograd?\n",
    "\n",
    "Autograd is PyTorch's automatic differentiation engine. It records operations on tensors and computes gradients automatically using backpropagation.\n",
    "\n",
    "### Basic Gradient Computation\n",
    "\n",
    "```python\n",
    "# Enable gradient tracking\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "print(f\"Requires grad: {x.requires_grad}\")\n",
    "\n",
    "# Perform operations\n",
    "y = x ** 2  # y = x^2\n",
    "z = y.sum()  # z = sum(y)\n",
    "\n",
    "print(f\"y requires grad: {y.requires_grad}\")  # True (inherited)\n",
    "print(f\"z requires grad: {z.requires_grad}\")  # True\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()  # Computes dz/dx\n",
    "\n",
    "print(f\"Gradient: {x.grad}\")  # [4.0, 6.0] since dz/dx = 2x\n",
    "```\n",
    "\n",
    "### Gradient Flow\n",
    "\n",
    "```python\n",
    "# Example: f(x) = (x^2 + 3x)^2\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "a = x ** 2  # a = x^2\n",
    "b = 3 * x  # b = 3x\n",
    "c = a + b  # c = x^2 + 3x\n",
    "y = c ** 2  # y = (x^2 + 3x)^2\n",
    "\n",
    "# Backward pass\n",
    "y.backward()\n",
    "\n",
    "print(f\"dy/dx = {x.grad}\")  # Computed using chain rule\n",
    "```\n",
    "\n",
    "### Controlling Gradient Computation\n",
    "\n",
    "```python\n",
    "# Detach from computational graph\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "z = y.detach()  # z is now a regular tensor, no gradient tracking\n",
    "\n",
    "# Context manager: no gradient tracking\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x ** 2  # No gradient will be computed\n",
    "    print(y.requires_grad)  # False\n",
    "\n",
    "# Inference mode (more efficient than no_grad)\n",
    "with torch.inference_mode():\n",
    "    y = x ** 2\n",
    "\n",
    "# Enable/disable gradient tracking\n",
    "x = torch.rand(3, 4)\n",
    "x.requires_grad_(True)  # Enable\n",
    "x.requires_grad_(False)  # Disable\n",
    "```\n",
    "\n",
    "### Computing Higher-Order Derivatives\n",
    "\n",
    "```python\n",
    "# Second derivative\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x ** 3  # y = x^3\n",
    "\n",
    "# First derivative\n",
    "dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "print(f\"dy/dx = {dy_dx}\")  # 3x^2 = 12\n",
    "\n",
    "# Second derivative\n",
    "d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]\n",
    "print(f\"d²y/dx² = {d2y_dx2}\")  # 6x = 12\n",
    "```\n",
    "\n",
    "### Custom Gradient Functions\n",
    "\n",
    "```python\n",
    "class CustomFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Save for backward\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)  # ReLU\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "# Usage\n",
    "custom_relu = CustomFunction.apply\n",
    "x = torch.randn(5, requires_grad=True)\n",
    "y = custom_relu(x)\n",
    "y.sum().backward()\n",
    "```\n",
    "\n",
    "### Gradient Accumulation\n",
    "\n",
    "```python\n",
    "# Gradients accumulate by default\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "for i in range(3):\n",
    "    y = x ** 2\n",
    "    y.backward()\n",
    "    print(f\"Iteration {i+1}, gradient: {x.grad}\")\n",
    "\n",
    "# Reset gradients\n",
    "x.grad.zero_()  # or x.grad = None\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Neural Networks with torch.nn\n",
    "\n",
    "### Building Blocks\n",
    "\n",
    "#### Linear Layer (Fully Connected)\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear layer: y = xW^T + b\n",
    "linear = nn.Linear(in_features=10, out_features=5)\n",
    "\n",
    "# Parameters\n",
    "print(f\"Weight shape: {linear.weight.shape}\")  # (5, 10)\n",
    "print(f\"Bias shape: {linear.bias.shape}\")  # (5,)\n",
    "\n",
    "# Forward pass\n",
    "x = torch.rand(3, 10)  # Batch of 3 samples\n",
    "output = linear(x)  # shape: (3, 5)\n",
    "```\n",
    "\n",
    "#### Activation Functions\n",
    "\n",
    "```python\n",
    "# ReLU: max(0, x)\n",
    "relu = nn.ReLU()\n",
    "x = torch.tensor([-1.0, 0.0, 1.0])\n",
    "output = relu(x)  # [0.0, 0.0, 1.0]\n",
    "\n",
    "# Leaky ReLU: max(0.01x, x)\n",
    "leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "# Sigmoid: 1 / (1 + e^(-x))\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "# Tanh: (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "tanh = nn.Tanh()\n",
    "\n",
    "# Softmax: e^xi / sum(e^xj)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "x = torch.rand(2, 5)\n",
    "output = softmax(x)  # Each row sums to 1\n",
    "\n",
    "# GELU (used in transformers)\n",
    "gelu = nn.GELU()\n",
    "\n",
    "# Swish/SiLU: x * sigmoid(x)\n",
    "silu = nn.SiLU()\n",
    "```\n",
    "\n",
    "#### Convolutional Layers\n",
    "\n",
    "```python\n",
    "# 2D Convolution\n",
    "conv2d = nn.Conv2d(\n",
    "    in_channels=3,      # RGB input\n",
    "    out_channels=64,    # 64 filters\n",
    "    kernel_size=3,      # 3x3 kernel\n",
    "    stride=1,           # Stride\n",
    "    padding=1,          # Padding\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "# Input: (batch, channels, height, width)\n",
    "x = torch.rand(8, 3, 32, 32)  # 8 RGB images of 32x32\n",
    "output = conv2d(x)  # shape: (8, 64, 32, 32)\n",
    "\n",
    "# 1D Convolution (for sequences)\n",
    "conv1d = nn.Conv1d(\n",
    "    in_channels=10,\n",
    "    out_channels=20,\n",
    "    kernel_size=3\n",
    ")\n",
    "\n",
    "# 3D Convolution (for video/volumetric data)\n",
    "conv3d = nn.Conv3d(\n",
    "    in_channels=1,\n",
    "    out_channels=32,\n",
    "    kernel_size=3\n",
    ")\n",
    "\n",
    "# Transposed Convolution (upsampling)\n",
    "deconv = nn.ConvTranspose2d(\n",
    "    in_channels=64,\n",
    "    out_channels=3,\n",
    "    kernel_size=4,\n",
    "    stride=2,\n",
    "    padding=1\n",
    ")\n",
    "```\n",
    "\n",
    "#### Pooling Layers\n",
    "\n",
    "```python\n",
    "# Max Pooling\n",
    "maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "x = torch.rand(1, 1, 4, 4)\n",
    "output = maxpool(x)  # shape: (1, 1, 2, 2)\n",
    "\n",
    "# Average Pooling\n",
    "avgpool = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "# Adaptive Pooling (output size specified)\n",
    "adaptive_maxpool = nn.AdaptiveMaxPool2d(output_size=(7, 7))\n",
    "x = torch.rand(1, 512, 14, 14)\n",
    "output = adaptive_maxpool(x)  # shape: (1, 512, 7, 7)\n",
    "\n",
    "# Global Average Pooling\n",
    "gap = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "x = torch.rand(1, 512, 7, 7)\n",
    "output = gap(x)  # shape: (1, 512, 1, 1)\n",
    "```\n",
    "\n",
    "#### Normalization Layers\n",
    "\n",
    "```python\n",
    "# Batch Normalization\n",
    "batch_norm = nn.BatchNorm2d(num_features=64)\n",
    "x = torch.rand(8, 64, 32, 32)\n",
    "output = batch_norm(x)\n",
    "\n",
    "# Layer Normalization\n",
    "layer_norm = nn.LayerNorm(normalized_shape=[64, 32, 32])\n",
    "\n",
    "# Instance Normalization\n",
    "instance_norm = nn.InstanceNorm2d(num_features=64)\n",
    "\n",
    "# Group Normalization\n",
    "group_norm = nn.GroupNorm(num_groups=8, num_channels=64)\n",
    "```\n",
    "\n",
    "#### Dropout\n",
    "\n",
    "```python\n",
    "# Regular Dropout\n",
    "dropout = nn.Dropout(p=0.5)  # Drop 50% of neurons\n",
    "x = torch.rand(10, 100)\n",
    "output = dropout(x)\n",
    "\n",
    "# 2D Dropout (for convolutional layers)\n",
    "dropout2d = nn.Dropout2d(p=0.5)\n",
    "\n",
    "# Alpha Dropout (for SELU activation)\n",
    "alpha_dropout = nn.AlphaDropout(p=0.5)\n",
    "```\n",
    "\n",
    "#### Recurrent Layers\n",
    "\n",
    "```python\n",
    "# LSTM\n",
    "lstm = nn.LSTM(\n",
    "    input_size=10,      # Input feature size\n",
    "    hidden_size=20,     # Hidden state size\n",
    "    num_layers=2,       # Number of stacked LSTMs\n",
    "    batch_first=True,   # Input shape: (batch, seq, features)\n",
    "    dropout=0.5,        # Dropout between layers\n",
    "    bidirectional=False\n",
    ")\n",
    "\n",
    "# Input: (batch, sequence_length, features)\n",
    "x = torch.rand(5, 30, 10)  # 5 sequences of length 30\n",
    "output, (h_n, c_n) = lstm(x)\n",
    "print(f\"Output shape: {output.shape}\")  # (5, 30, 20)\n",
    "print(f\"Hidden state shape: {h_n.shape}\")  # (2, 5, 20)\n",
    "\n",
    "# GRU (Gated Recurrent Unit)\n",
    "gru = nn.GRU(\n",
    "    input_size=10,\n",
    "    hidden_size=20,\n",
    "    num_layers=2,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "# Simple RNN\n",
    "rnn = nn.RNN(\n",
    "    input_size=10,\n",
    "    hidden_size=20,\n",
    "    num_layers=2,\n",
    "    batch_first=True\n",
    ")\n",
    "```\n",
    "\n",
    "#### Attention Mechanisms\n",
    "\n",
    "```python\n",
    "# Multi-Head Attention\n",
    "multihead_attn = nn.MultiheadAttention(\n",
    "    embed_dim=512,      # Embedding dimension\n",
    "    num_heads=8,        # Number of attention heads\n",
    "    dropout=0.1,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "# Query, Key, Value\n",
    "Q = torch.rand(10, 32, 512)  # (batch, seq_len, embed_dim)\n",
    "K = torch.rand(10, 32, 512)\n",
    "V = torch.rand(10, 32, 512)\n",
    "\n",
    "attn_output, attn_weights = multihead_attn(Q, K, V)\n",
    "print(f\"Output shape: {attn_output.shape}\")  # (10, 32, 512)\n",
    "```\n",
    "\n",
    "#### Transformer Components\n",
    "\n",
    "```python\n",
    "# Transformer Encoder Layer\n",
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=512,        # Embedding dimension\n",
    "    nhead=8,            # Number of heads\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "# Transformer Encoder (stack of encoder layers)\n",
    "transformer_encoder = nn.TransformerEncoder(\n",
    "    encoder_layer,\n",
    "    num_layers=6\n",
    ")\n",
    "\n",
    "# Input\n",
    "src = torch.rand(10, 32, 512)  # (batch, seq_len, d_model)\n",
    "output = transformer_encoder(src)\n",
    "\n",
    "# Transformer Decoder Layer\n",
    "decoder_layer = nn.TransformerDecoderLayer(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    dim_feedforward=2048,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "# Complete Transformer\n",
    "transformer = nn.Transformer(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    batch_first=True\n",
    ")\n",
    "```\n",
    "\n",
    "### Building Custom Networks\n",
    "\n",
    "#### Method 1: Sequential\n",
    "\n",
    "```python\n",
    "# Simple feedforward network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "x = torch.rand(32, 784)\n",
    "output = model(x)\n",
    "```\n",
    "\n",
    "#### Method 2: nn.Module (Recommended)\n",
    "\n",
    "```python\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(CustomNet, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define forward pass\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate\n",
    "model = CustomNet(input_size=784, hidden_size=256, num_classes=10)\n",
    "\n",
    "# Forward pass\n",
    "x = torch.rand(32, 784)\n",
    "output = model(x)\n",
    "```\n",
    "\n",
    "#### Complex Architecture Example: ResNet Block\n",
    "\n",
    "```python\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, \n",
    "                               kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
    "                               kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels,\n",
    "                          kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Add shortcut\n",
    "        out += self.shortcut(identity)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "```\n",
    "\n",
    "### Model Inspection\n",
    "\n",
    "```python\n",
    "model = CustomNet(784, 256, 10)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "# Access specific layers\n",
    "print(model.fc1.weight.shape)\n",
    "print(model.fc1.bias.shape)\n",
    "\n",
    "# Iterate over parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n",
    "# Iterate over modules\n",
    "for name, module in model.named_modules():\n",
    "    print(f\"{name}: {type(module)}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Optimization\n",
    "\n",
    "### Optimizers\n",
    "\n",
    "```python\n",
    "model = CustomNet(784, 256, 10)\n",
    "\n",
    "# SGD (Stochastic Gradient Descent)\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.01,            # Learning rate\n",
    "    momentum=0.9,       # Momentum\n",
    "    weight_decay=1e-4,  # L2 regularization\n",
    "    nesterov=True       # Nesterov momentum\n",
    ")\n",
    "\n",
    "# Adam (Adaptive Moment Estimation)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0\n",
    ")\n",
    "\n",
    "# AdamW (Adam with decoupled weight decay)\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# RMSprop\n",
    "optimizer = optim.RMSprop(\n",
    "    model.parameters(),\n",
    "    lr=0.01,\n",
    "    alpha=0.99,\n",
    "    momentum=0\n",
    ")\n",
    "\n",
    "# Adagrad\n",
    "optimizer = optim.Adagrad(\n",
    "    model.parameters(),\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# Adadelta\n",
    "optimizer = optim.Adadelta(\n",
    "    model.parameters(),\n",
    "    lr=1.0,\n",
    "    rho=0.9\n",
    ")\n",
    "```\n",
    "\n",
    "### Learning Rate Schedulers\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step decay\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=30,  # Decay every 30 epochs\n",
    "    gamma=0.1      # Multiply lr by 0.1\n",
    ")\n",
    "\n",
    "# Multi-step decay\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=[30, 80, 120],\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "# Exponential decay\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma=0.95\n",
    ")\n",
    "\n",
    "# Cosine annealing\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=100,  # Maximum number of iterations\n",
    "    eta_min=0   # Minimum learning rate\n",
    ")\n",
    "\n",
    "# Reduce on plateau\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',       # min for loss, max for accuracy\n",
    "    factor=0.5,       # Multiply lr by 0.5\n",
    "    patience=10,      # Wait 10 epochs\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Cosine annealing with warm restarts\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,    # Initial restart period\n",
    "    T_mult=2,  # Multiply period by 2 after each restart\n",
    "    eta_min=0\n",
    ")\n",
    "\n",
    "# One Cycle Policy ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
